 Advanced Macro I Fall 2009
Miklós Koren korenm@ceu.hu koren.hu/teaching

 Lecture 13: Dynamic programming in continuous time

 Outline

 Outline
􏰈 Today we start using our tools for dynamic programming.
􏰈 no uncertainty
􏰈 Markov chains
􏰈 We review some simple applications.
1

 From last time

 Solutions to the teamwork assignments
􏰈 Both CEU email servers had the following stationary distribution:
π n∗ = 1 / N
for n = 0, 1, ..., N − 1 and 0 thereafter.
􏰈 It was easiest to see for server 2, because the Kolmogorov equation gave πn∗ = πn∗−1 and π0∗ = πN∗ −1.
2

 Solutions to the teamwork assignments
􏰈 For server 1,
λπN∗ −1 = (λ + η)πN∗ so the stationary distribution is
if n < N if n = N
􏰓 λ+η N(λ+η)+λ
 πn∗ =
􏰈 Takeη→∞togettheresult.
λ
N (λ+η)+λ
 3

 Discrete time

 Dynamic programming with no uncertainty
􏰈 Suppose you have a vector of state variables xt, and control variables ct.
􏰈 The equation of motion for the state is ∆xt+1 =F(xt,ct).
􏰈 Per-period utility is
u(xt, ct)
􏰈 The sequential problem is
∞
max 􏰔 βtu(xt, ct) s.t. ∆xt+1 = xt + F (xt, ct) {ct} t=1
4

 The recursive formulation
􏰈 The corresponding Bellman equation is
V (xt) = max {u(xt, ct) + βV (xt+1)}
ct
􏰈 or substituting in the equation of motion
V(xt)=max{u(xt,ct)+βV[xt +F(xt,ct)]} ct
5

 Solution
􏰈 The solution is a value function V (x) that maps the state into the PDV of utility.
􏰈 Equivalently, the solution can be given as a policy function c(x).
􏰈 What would change if time periods were days instead of years?
6

 Continuous time

 Moving to continuous time
􏰈 Let time periods be ∆ apart.
􏰈 As before, we want to characterize the time series as ∆ becomes smaller and smaller.
􏰈 Wetakethelimitas∆→0.
􏰈 We will have to rescale 􏰌ows, but not stocks.
7

 Di􏰉erential equations
􏰈 Now dynamics are characterized by the di􏰉erential equation:
x ̇(t) = lim F(xt,ct,∆t) ≡ f(xt,ct). ∆t→0 ∆t
 8

 Dynamic programming
􏰈 Back to our discrete-time Bellman: V(xt)=max{u(xt,ct)+βV[xt +F(xt,ct)]}
ct
􏰈 Which of the objects here depend on the length of the time period?
9

 Dynamic programming
􏰈 Back to our discrete-time Bellman: V(xt)=max{u(xt,ct)+βV[xt +F(xt,ct)]}
ct
􏰈 Which of the objects here depend on the length of the time
period?
􏰈 u, because utility is a 􏰌ow: shorter time periods yield less utility
􏰈 β, because shorter time periods are discounted less
􏰈 F as we have seen above
9

 Dynamic programming
􏰈 Back to our discrete-time Bellman: V(xt)=max{u(xt,ct)+βV[xt +F(xt,ct)]}
ct
􏰈 Which of the objects here depend on the length of the time
period?
􏰈 u, because utility is a 􏰌ow: shorter time periods yield less utility
􏰈 β, because shorter time periods are discounted less
􏰈 F as we have seen above
􏰈 Let us make this period dependence explicit: 􏰑1􏰒
V(xt)=max u(xt,ct)∆+ V[xt +f(xt,ct)∆] ct 1+ρ∆
􏰈 Now u is the per-period utility, ρ is the per-period discount rate, f is the per-period growth rate.
  9

 In􏰊nitesimal periods
As you might expect, we take ∆ to 0. First multiply by (1 + ρ∆):
(1+ρ∆)V(xt)=max{u(xt,ct)∆(1+ρ∆)+V[xt +f(xt,ct)∆]} ct
Then subtract V (xt):
ρ∆V(xt)=max{u(xt,ct)∆(1+ρ∆)+V[xt +f(xt,ct)∆]−V(xt)}
ct Then divide by ∆:
 􏰑 V[xt +f(xt,ct)∆]−V(xt)􏰒 u(xt, ct)(1 + ρ∆) +
ρV (xt) = max
ct ∆
 Now we're ready to take the limit.
10

 The Hamilton-Jacobi-Bellman equation
ρV (xt ) = max 􏰍u(xt , ct ) + V ′ (xt )f (xt , ct )􏰎 ct
􏰈 What is di􏰉erent:
􏰈 wehaveρV,notV ontheLHS
􏰈 we have V ̇, not new value on RHS
􏰈 Intuition:
􏰈 the per-period discount loss from my value should be
compensated by
􏰈 􏰌ow utility
􏰈 and (expected) gains in future value
 11

 Teamwork: eat-the-pie problem
 􏰈 You have wealth W, accruing interest r per unit of time.
􏰈 You maximize
􏰕∞
exp(−ρt) ln c(t)dt.
t=0
1. Write down the Bellman equation.
2. GuessthatV(W)=a+blnW andsolveforaandb. 3. What is the optimal consumption policy, c(W)?
12

 Application to the Ramsey model
􏰈 Let
xt = kt,
u(xt, ct) = c1−θ/(1 − θ),
t f(xt,ct)=f(kt)−δkt −ct
􏰈 The Bellman equation is now
􏰑 c1−θ ′ 􏰒
ρV(k)=max +V (k)[f(k)−δk−c] c 1−θ
 13

 Deriving the Euler equation
􏰈 The FOC for optimal c:
c−θ = V ′(k).
􏰈 Taking logs and di􏰉erentiating wrt time −θcˆ= V′′(k)[f(k)−δk−c]
V′(k)
(xˆ denotes x ̇/x)
􏰈 Di􏰉erentiating through the Bellman to express V ′(k):
ρV ′(k) = V ′′(k)[f(k) − δk − c] + V ′(k)[f′(k) − δ]
􏰈 SubstitutinginV′′(k):
cˆ= 1[f′(k)−δ−ρ]≡ 1[r(k)−ρ] θθ
   14

 The consumption rule
cˆ= 1[f′(k)−δ−ρ]≡ 1[r(k)−ρ] θθ
􏰈 Agents like to smooth consumption (especially with high θ).
􏰈 Consumption grows if r > ρ: the market return on my saving (late consumption) is higher than the private return from early consumption.
􏰈 Consumption growth is high if k is low (r is high).
  15

 Uncertainty
 
 Moments of a Markov chain
􏰈 Takeafunctionv()thatassignseachstateavalue,v1,...,vN.
􏰈 What is the expected value of v in a short period of time if we
are in state si now?
􏰈 If ∆ ≈ 0, the probability of moving from the state is ≈ 0, so
the expected value is ≈ vi.
􏰈 It is more meaningful to talk about the "rate" of change, as in
the case of di􏰉erential equations.
􏰈 What is the expected rate of change in v?
E(dv) = lim E(vt+∆|t) − vt dt ∆→0 ∆
  16

 Expectations
􏰈 Again, start from discrete time: E[v(t+∆)|t]=(1−􏰔∆λij)vi +􏰔∆λijvj
j ̸=i j ̸=i
􏰈 Subtract vi = v(t) from both sides and divide by ∆:
E[v(t+∆)|t]−v(t) =􏰔λij(vj −vi)
∆ j̸=i
􏰈 Taking∆→0
E(dv) =􏰔λij(vj −vi) dt j̸=i
􏰈 Intuitively, the expected change is a weighted average of all potential changes.
􏰈 A change of vj − vi arrives with arrival rate λij , so has a weight λij.
   17

 Dynamic programming
􏰈 We can derive the continuous-time HJB equation with uncertainty as
􏰏 EdV(x)􏰐 ρV(xi)=max u(xi,c)+
 c dt
􏰈 The only change is that we have expected change in V on the RHS.
1. This holds if x is a jump process. 2. x is continuous
3. xisamixofthetwo
18

 Dynamic programming
􏰈 Suppose our state x follows a Markov chain as above.
􏰈 Arrival rates λij may depend on past states (i) and on the
policy variable c.
􏰈 Because x is a jump process,
EdV(x) =􏰔λij[V(xj)−V(xi)].
dt j̸=i
􏰈 The HJB equation simpli􏰊es to 
ρV (xi ) = max u(xi , c) + 􏰔 λj (xi , c)[V (xj ) − V (xi )] c j̸=i 
 19

 Dynamic programming
􏰈 More generally, suppose x follows a Markov chain, and y is a continuous state variable.
􏰈 The HJB equation can be written as ρV (xi, y) =
 maxu(xi,y,c)+􏰔λj(xi,y,c)[V(xj,y)−V(xi,y)]
c j̸=i  +f(xi,y,c)Vy(xi,y)
 20

 Dynamic programming
􏰈 More generally, suppose x follows a Markov chain, and y is a continuous state variable.
􏰈 The HJB equation can be written as ρV (xi, y) =
 maxu(xi,y,c)+􏰔λj(xi,y,c)[V(xj,y)−V(xi,y)]
 􏰈
􏰈 􏰈
c j̸=i  +f(xi,y,c)Vy(xi,y)
This only applies if y does not change when x jumps. 􏰈 y is an aggregate variable, x is individual
Otherwise the jump would have to be accounted for. The reverse does not matter. Why?
20

 Checklist
By now you should understand
1. forward Kolmogorov equation 2. stationary distribution
3. Poisson process
4. Poisson distribution
5. moments of a Markov chain
6. Hamilton􏰋Jacobi􏰋Bellman equation with
􏰈 continuous deterministic states
􏰈 jump processes
21

 Applications

 Applications
􏰈 We consider three simple applications
1. A model of exogenous job loss and job 􏰊nding.
2. A model of endogenous job search. 3. A model of a milk farm.
22

 Exogenous job search

 Exogenous job search
􏰈 There are only two states:
􏰈 E1: worker has a job
􏰈 E2: worker is unemployed
􏰈 Transition across states (job loss, job 􏰊nding) is exogenous (for now).
􏰈 No optimization involved.
􏰈 This is just a simple way of calculating the NPV of a job.
23

 Exogenous job search
􏰈 The hazard rate of losing a job is δ.
􏰈 The arrival rate of a new job for an unemployed is λ.
24

 Exogenous job search
􏰈 The per-period value of holding a job is w.
􏰈 The per-period value of being unemployed is b.
􏰈 What is the overall (present discounted) value of a job?
􏰕∞ t=0
􏰈 St is random and varies over time.
􏰈 We may have to evaluate a complex integral.
􏰈 Dynamic programming makes our lives easier.
VJ =
exp(−ρt)u(St).
25

 The Bellman equation
􏰈 The Bellman equation characterizing the value of a job ρVJ =w+δ(VU −VJ).
􏰈 The Bellman equation characterizing the value of unemployment
ρVU =b+λ(VJ −VU).
􏰈 Note that you can think of the value function as V (s) with s
taking only two values.
􏰈 This looks much simpler! A system of two linear equations with two unknowns.
26

 Solution
􏰈 The solution is
1􏰏ρ+λδ􏰐 VJ=ρ ρ+λ+δw+ρ+λ+δb
1􏰏ρ+δλ􏰐 VU =ρ ρ+λ+δb+ρ+λ+δw
􏰈 The value of a job is the weighted average of the PDV of wages and the PDV of bene􏰊ts.
􏰈 The weigths depend on all parameters.
       27

 Comparative statics
􏰈 The 􏰈 􏰈 􏰈
value of a job is increasing in wages
bene􏰊ts
job 􏰊nding rate
􏰈 Decreasing in
􏰈 discount rate
􏰈 􏰊ring rate
􏰈 Converges to the PDV of wages w/ρ as
􏰈 δ goes to zero
􏰈 λ goes to in􏰊nity
28

 Endogenous job search

 Endogenous job search
􏰈 We now endogenize job search. Everything else remains the same.
􏰈 To make sure that jobs arrive at rate λ, the unemployed has to pay a search cost g(λ).
􏰈 g is increasing, convex, twice di􏰉erentiable, Inada
􏰈 Note that g(λ) is a 􏰌ow: the search e􏰉ort at a given moment
in time.
􏰈 λ is also a 􏰌ow: the probability of success at a given moment in time.
􏰈 This still a memoryless process:
􏰈 past search e􏰉orts have no e􏰉ect
􏰈 (just as time has no e􏰉ect)
 29

 The Bellman equation
􏰈 The Bellman is now
ρVJ =w+δ(VU −VJ)
ρVU =max[b−g(λ)+λ(VJ −VU)] λ
􏰈 Note the maximization in the unemployed state.
30

 First-order condition
􏰈 TheFOCforλis
g′(λ∗)=VJ −VU
􏰈 The "exogenous" Bellman still correctly calculates VJ and VU once we substitute in the new bene􏰊ts b − g(λ∗) and the job 􏰊nding rate λ∗.
􏰈 Find a λ∗ that satis􏰊es both the FOC and the Bellman.
31

 Solution
􏰈 Subtracting the two Bellman equations:
VJ − VU = w − b + g(λ∗)
ρ + δ + λ∗
􏰈 Substitute this into FOC to get an implicit solution for λ∗:
g′(λ∗)= w−b+g(λ∗) ρ + δ + λ∗
   32

 Comparative statics
􏰈 Totally di􏰉erentiating the implicit function... 􏰈 search intensity λ∗ is
􏰈 increasing in w − b
􏰈 decreasing in ρ + δ
􏰈 decreasing with an upward shift of search costs
33

 A milk farm
􏰈 Our last application considers a milk farm with n cows. 􏰈 The only state variable is n.
􏰈 The control variable is the feed c we give to each cow.
34

 Flow pro􏰊ts
􏰈 Each cow gives m(c) milk per period of time. 􏰈 m is increasing and concave.
􏰈 Flow pro􏰊ts are
where
π = [pm(c) − wc]n,
􏰈 p is the price of milk
􏰈 w is the price of feed
35

 Dynamics
􏰈 New cows are born at rate λ to each existing cow (not modeled).
􏰈 Cows die at rate δ(c).
􏰈 δ is decreasing and convex
􏰈 What is the dynamics of n?
􏰈 Over a ∆ period of time, n becomes
􏰈 n + 1 with probability λn∆
􏰈 n − 1 with probability δ(c)n∆
􏰈 n with probability 1 − [λ + δ(c)]∆
36

 Valuing the farm
􏰈 What is the value of the farm? 􏰈 The Bellman equation
ρV (n) = max{[pm(c) − wc]n+ c
λn[V(n+1)−V(n)]+
δ(c)n[V (n − 1) − V (n)]}
37

 Solution
􏰈 GuessthatV(n)=vn.
ρvn = max{[pm(c) − wc]n + λn[v] + δ(c)n[−v]}
c
ρv = max{pm(c) − wc + [λ − δ(c)]v} c
􏰈 FOCforc
􏰈 Find c and v so that both are satis􏰊ed.
or
pm′(c) − w − vδ′(c) = 0
38

 Appendix
 
 Empty slides for notes
39

 Empty slides for notes
40

 Empty slides for notes
 41

 Empty slides for notes
42
