%./preamble.tex
= Lecture 15: Dynamic programming in continuous time =
= Outline =
== Outline ==
* Today we start using our tools for dynamic programming.
    * no uncertainty
    * Markov chains
* We review some simple applications.


= From last time =
== Solutions to examples ==
* Both CEU email servers had the following stationary distribution:
\[
\pi_n^* = 1/N
\]
for $n=0,1,...,N-1$ and $0$ thereafter.
* It was easiest to see for server 2, because the Kolmogorov equation gave $\pi_n^*=\pi_{n-1}^*$ and
$\pi_0^*=\pi_{N-1}^*$.

== Solutions to the teamwork assignments ==
* For server 1,
\[
\lambda \pi_{N-1}^*=(\lambda+\eta)\pi_{N}^*
\]
so the stationary distribution is
\[
\pi_n^* = \begin{cases}
\frac{\lambda+\eta}{N(\lambda+\eta)+\lambda} & \text{if }n<N\\
\frac{\lambda}{N(\lambda+\eta)+\lambda} & \text{if }n=N
\end{cases}
\]
* Take $\eta\to\infty$ to get the result.


= Discrete time =
== Dynamic programming with no uncertainty ==
* Suppose you have a vector of state variables $x_t$, and control variables $c_t$.
* The equation of motion for the state is
\[
\Delta x_{t+1} = F(x_t,c_t).
\]
* Per-period utility is
\[
u(x_t,c_t)
\]
* The sequential problem is
\[
\max_{\{c_t\}} \sum_{t=1}^\infty \beta^t u(x_t,c_t)\text{  s.t.  }\Delta x_{t+1} = x_t+F(x_t,c_t)
\]

== The recursive formulation ==
* The corresponding Bellman equation is
\[
V(x_t) = \max_{c_t} \left\{u(x_t,c_t) + \beta V(x_{t+1})\right\}
\]
* or substituting in the equation of motion
\[
V(x_t) = \max_{c_t} \left\{u(x_t,c_t) + \beta V[x_t+F(x_{t},c_t)]\right\}
\]

== Solution ==
* The solution is a value function $V(x)$ that maps the state into the PDV of utility.
* Equivalently, the solution can be given as a policy function $c(x)$.
* What would change if time periods were days instead of years?



= Continuous time =
== Moving to continuous time ==
* Let time periods be $\Delta$ apart.
* As before, we want to characterize the time series as $\Delta $ becomes smaller and smaller.
* We take the limit as $\Delta \to0$.
    * We will have to rescale flows, but not stocks.

== Differential equations ==
* Now dynamics are characterized by the differential equation:
\[
\dot x(t) = \lim_{\Delta t\to0}\frac{F(x_{t},c_t,\Delta t)}{\Delta t} \equiv f(x_t,c_t).
\]

== Dynamic programming ==
* Back to our discrete-time Bellman:
\[
V(x_t) = \max_{c_t} \left\{u(x_t,c_t) + \beta V[x_t+F(x_{t},c_t)]\right\}
\]
* Which of the objects here depend on the length of the time period?
...
  * $u$, because utility is a ''flow'': shorter time periods yield less utility
  * $\beta$, because shorter time periods are discounted less
  * $F$ as we have seen above
...
* Let us make this period dependence explicit:
\[
V(x_t) = \max_{c_t} \left\{ u(x_t,c_t)\Delta  + \frac{1}{1+\rho \Delta } V[x_t+f(x_{t},c_t)\Delta ]\right\}
\]
* Now $u$ is the ''per-period'' utility, $\rho$ is the ''per-period'' discount rate, $f$ is the ''per-period'' growth rate.

== Infinitesimal periods ==
As you might expect, we take $\Delta$ to $0$.

First multiply by $(1+\rho \Delta)$:
\[
(1+\rho \Delta )V(x_t) = \max_{c_t} \left\{ u(x_t,c_t)\Delta (1+\rho \Delta ) + V[x_t+f(x_{t},c_t)\Delta ]\right\}
\]
Then subtract $V(x_t)$:
\[
\rho \Delta V(x_t) = \max_{c_t} \left\{ u(x_t,c_t)\Delta (1+\rho \Delta ) + V[x_t+f(x_{t},c_t)\Delta ]-V(x_t)\right\}
\]
Then divide by $\Delta $:
\[
\rho V(x_t) = \max_{c_t} \left\{ u(x_t,c_t) (1+\rho \Delta ) + \frac{V[x_t+f(x_{t},c_t)\Delta ]-V(x_t)}{\Delta }\right\}
\]
Now we're ready to take the limit.

== The Hamilton-Jacobi-Bellman equation ==
\[
\rho V(x_t) = \max_{c_t} \left[ u(x_t,c_t) + V'(x_t)f(x_{t},c_t)\right]
\]
* What is different:
  * we have $\rho V$, not $V$ on the LHS
  * we have $\dot V$, not new value on RHS
* Intuition:
  * the per-period discount loss from my value should be compensated by
  * flow utility
  * and (expected) gains in future value

== Example: eat-the-pie problem ==
* You have wealth $W$, accruing interest $r$ per unit of time.
* You maximize
\[
\int_{t=0}^\infty \exp(-\rho t)\ln c(t) dt.
\]
    1. Write down the Bellman equation.
    2. Guess that $V(W)=a+b \ln W$ and solve for $a$ and $b$.
    3. What is the optimal consumption policy, $c(W)$?

%% more examples here?

== Application to the Ramsey model ==
* Let
\begin{align*}
x_t &=k_t,\\
u(x_t,c_t) &= c_t^{1-\theta}/(1-\theta),\\
f(x_t,c_t) &= f(k_t)-\delta k_t-c_t
\end{align*}
* The Bellman equation is now
\[
\rho V(k) = \max_c\left\{
\frac{c^{1-\theta}}{1-\theta} + V'(k)[f(k)-\delta k-c]
\right\}
\]

== Deriving the Euler equation ==
* The FOC for optimal $c$:
\[
c^{-\theta} = V'(k).
\]
* Taking logs and differentiating wrt time
\[
-\theta \hat c = \frac{V'{}'(k)}{V'(k)}[f(k)-\delta k-c]
\]
($\hat x$ denotes $\dot x/x$)
* Differentiating through the Bellman to express $V'(k)$:
\[
\rho V'(k) =  V'{}'(k)[f(k)-\delta k-c] + V'(k)[f'(k)-\delta]
\]
* Substituting in $V'{}'(k)$:
\[
\hat c = \frac1\theta[f'(k)-\delta-\rho] \equiv \frac1\theta[r(k)-\rho]
\]

== The consumption rule ==
\[
\hat c = \frac1\theta[f'(k)-\delta-\rho] \equiv \frac1\theta[r(k)-\rho]
\]
* Agents like to smooth consumption (especially with high $\theta$).
* Consumption grows if $r>\rho$: the market return on my saving (late consumption) is higher than the private return from early consumption.
* Consumption growth is high if $k$ is low ($r$ is high).


% phase diagram
